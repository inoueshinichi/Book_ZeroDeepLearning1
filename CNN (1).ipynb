{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 28, 28)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4次元データ\n",
    "import numpy as np\n",
    "x = np.random.rand(10, 1, 28, 28)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39306944, 0.39258221, 0.83755357, 0.54253722, 0.46708034,\n",
       "        0.67089171, 0.71831255, 0.98025213, 0.47761512, 0.25901481,\n",
       "        0.32584829, 0.56325975, 0.12899117, 0.81693293, 0.79087862,\n",
       "        0.21698993, 0.07628293, 0.67216739, 0.80352697, 0.27084547,\n",
       "        0.87004438, 0.84546502, 0.92374841, 0.10332397, 0.14655203,\n",
       "        0.5357175 , 0.65938462, 0.3651124 ],\n",
       "       [0.8737905 , 0.80050039, 0.46035195, 0.47596828, 0.2932048 ,\n",
       "        0.27508998, 0.89579936, 0.08761305, 0.24121804, 0.90199842,\n",
       "        0.24305512, 0.91273664, 0.77925368, 0.75826825, 0.57212771,\n",
       "        0.78556193, 0.80827757, 0.67253085, 0.80873222, 0.76452365,\n",
       "        0.54466713, 0.99329643, 0.42893729, 0.74781068, 0.74581658,\n",
       "        0.53209211, 0.24946559, 0.11276948],\n",
       "       [0.18480603, 0.55858179, 0.64690382, 0.82518707, 0.42506482,\n",
       "        0.8522002 , 0.79969185, 0.97300105, 0.19442024, 0.43044254,\n",
       "        0.59377272, 0.63107475, 0.23738865, 0.16877247, 0.40211591,\n",
       "        0.57902983, 0.8877502 , 0.6476603 , 0.5638835 , 0.72965599,\n",
       "        0.6975959 , 0.51534402, 0.71764132, 0.23653495, 0.25804494,\n",
       "        0.08617135, 0.98924473, 0.5837518 ],\n",
       "       [0.37217398, 0.20743164, 0.27867405, 0.06383494, 0.65180746,\n",
       "        0.4532972 , 0.97557437, 0.92977364, 0.17056412, 0.6959239 ,\n",
       "        0.04874864, 0.6454332 , 0.81191044, 0.27705772, 0.05468431,\n",
       "        0.18364005, 0.39618267, 0.10386384, 0.45239629, 0.25890691,\n",
       "        0.18356672, 0.32013034, 0.6927883 , 0.25116179, 0.68431793,\n",
       "        0.29840273, 0.50562306, 0.00358084],\n",
       "       [0.99321915, 0.42433076, 0.73529165, 0.43375371, 0.12619707,\n",
       "        0.5041941 , 0.86441151, 0.67162389, 0.55095857, 0.97671001,\n",
       "        0.64971562, 0.89274839, 0.9481773 , 0.10615085, 0.26316268,\n",
       "        0.32713481, 0.22741028, 0.64619664, 0.75628362, 0.39369515,\n",
       "        0.34163677, 0.68126036, 0.01696531, 0.48781416, 0.49905383,\n",
       "        0.72020665, 0.61258551, 0.06428803],\n",
       "       [0.75900983, 0.75056402, 0.17677728, 0.91137679, 0.48637429,\n",
       "        0.65619348, 0.19690659, 0.33516684, 0.45175231, 0.34290077,\n",
       "        0.59678799, 0.33663725, 0.14767146, 0.56977976, 0.53516147,\n",
       "        0.56773597, 0.45644223, 0.26359598, 0.02921939, 0.67347806,\n",
       "        0.42055792, 0.20329792, 0.0961733 , 0.01735108, 0.4660925 ,\n",
       "        0.48086199, 0.51200715, 0.32754786],\n",
       "       [0.97706647, 0.97579347, 0.9409944 , 0.40447173, 0.74160436,\n",
       "        0.53021014, 0.4878983 , 0.0398228 , 0.81004129, 0.55258474,\n",
       "        0.98505082, 0.54045972, 0.96572042, 0.7059305 , 0.77872887,\n",
       "        0.28266732, 0.45319735, 0.12214337, 0.93486769, 0.14999516,\n",
       "        0.01393137, 0.06066451, 0.97691112, 0.83743534, 0.82204106,\n",
       "        0.16607344, 0.49933311, 0.96281057],\n",
       "       [0.89605882, 0.08764729, 0.63814637, 0.48720456, 0.59242567,\n",
       "        0.95091465, 0.98966337, 0.92022567, 0.18276685, 0.34386961,\n",
       "        0.51897668, 0.98706281, 0.66178048, 0.22418663, 0.75262047,\n",
       "        0.55296806, 0.54459629, 0.55272843, 0.56557695, 0.99009181,\n",
       "        0.88398343, 0.8926933 , 0.74592319, 0.68630013, 0.05660595,\n",
       "        0.11157119, 0.87802963, 0.13181524],\n",
       "       [0.42344858, 0.03592356, 0.82988415, 0.12099805, 0.95649507,\n",
       "        0.85463054, 0.07124645, 0.06196843, 0.11810631, 0.14656798,\n",
       "        0.83756221, 0.02666849, 0.38579528, 0.9341208 , 0.02808727,\n",
       "        0.90983948, 0.80650828, 0.5798725 , 0.23768152, 0.36900377,\n",
       "        0.16117234, 0.19109227, 0.68633363, 0.65380946, 0.72468616,\n",
       "        0.7378825 , 0.38025437, 0.66202944],\n",
       "       [0.77585409, 0.29445688, 0.873023  , 0.31889945, 0.55799844,\n",
       "        0.29144411, 0.48658807, 0.67623767, 0.64415023, 0.20779182,\n",
       "        0.11319113, 0.3932945 , 0.25070919, 0.54804198, 0.60972969,\n",
       "        0.02352969, 0.18891396, 0.4204364 , 0.02729602, 0.86140425,\n",
       "        0.56074399, 0.06420033, 0.56281254, 0.56822719, 0.48181492,\n",
       "        0.02008043, 0.81701897, 0.00177924],\n",
       "       [0.79913192, 0.85814404, 0.95036124, 0.91970971, 0.70832174,\n",
       "        0.88966137, 0.89038102, 0.22108277, 0.06468212, 0.75424549,\n",
       "        0.21772099, 0.58605485, 0.45650075, 0.93252494, 0.15817877,\n",
       "        0.2823605 , 0.06765025, 0.99852117, 0.03809276, 0.66888727,\n",
       "        0.01992348, 0.75695974, 0.97709914, 0.26571226, 0.12575196,\n",
       "        0.72063429, 0.98298088, 0.52541411],\n",
       "       [0.84906821, 0.34241868, 0.85503181, 0.54549863, 0.69719715,\n",
       "        0.78951057, 0.80067775, 0.2604484 , 0.10608031, 0.67965099,\n",
       "        0.31162024, 0.74023534, 0.46974836, 0.61619548, 0.02960398,\n",
       "        0.74905532, 0.80222755, 0.32119744, 0.09872589, 0.03003201,\n",
       "        0.87143938, 0.0805995 , 0.37367997, 0.14910235, 0.59654881,\n",
       "        0.79866858, 0.33614547, 0.18321159],\n",
       "       [0.26660727, 0.44368551, 0.07616313, 0.67088578, 0.54452786,\n",
       "        0.11268763, 0.70553036, 0.91910575, 0.06389702, 0.98997139,\n",
       "        0.79354419, 0.30853096, 0.2786451 , 0.47076926, 0.31554941,\n",
       "        0.38917584, 0.89204384, 0.19678213, 0.85562768, 0.55659197,\n",
       "        0.08491687, 0.46978414, 0.54873258, 0.58550764, 0.63994441,\n",
       "        0.41306361, 0.77530134, 0.16332751],\n",
       "       [0.3243344 , 0.72163193, 0.86924673, 0.73763078, 0.94759557,\n",
       "        0.30184675, 0.11660444, 0.52829023, 0.02335014, 0.80761023,\n",
       "        0.98298237, 0.99357983, 0.79043367, 0.4388291 , 0.04182651,\n",
       "        0.21605369, 0.61663237, 0.96499267, 0.16556262, 0.88690588,\n",
       "        0.28081606, 0.9937174 , 0.75258053, 0.86712029, 0.29736034,\n",
       "        0.94921923, 0.02603048, 0.02578234],\n",
       "       [0.76067811, 0.35818426, 0.84784071, 0.50428544, 0.32332607,\n",
       "        0.18061124, 0.97213644, 0.863735  , 0.93900586, 0.69742057,\n",
       "        0.68030623, 0.8828477 , 0.1610603 , 0.19215263, 0.60179126,\n",
       "        0.10316888, 0.59318821, 0.39805884, 0.42559317, 0.50520561,\n",
       "        0.48314297, 0.7927579 , 0.02216496, 0.53938751, 0.40998204,\n",
       "        0.88783835, 0.07454579, 0.33275782],\n",
       "       [0.6025084 , 0.89063935, 0.93266766, 0.04532368, 0.6580739 ,\n",
       "        0.17322123, 0.65701157, 0.27360397, 0.24766782, 0.67233977,\n",
       "        0.01010978, 0.36355344, 0.87869487, 0.92297827, 0.57303135,\n",
       "        0.35707746, 0.69777885, 0.40553655, 0.59584538, 0.80025435,\n",
       "        0.2005267 , 0.52002536, 0.92262331, 0.71861859, 0.31573761,\n",
       "        0.7074477 , 0.22332788, 0.16959121],\n",
       "       [0.86255829, 0.56160502, 0.85762708, 0.353386  , 0.90663863,\n",
       "        0.88241858, 0.09344496, 0.30924165, 0.47483285, 0.24832064,\n",
       "        0.03367088, 0.53859253, 0.59015103, 0.11498271, 0.93278221,\n",
       "        0.3406003 , 0.52765874, 0.32421418, 0.43546322, 0.2086732 ,\n",
       "        0.50972322, 0.61003432, 0.39954992, 0.24851654, 0.28396383,\n",
       "        0.35824519, 0.18678092, 0.77287317],\n",
       "       [0.80576045, 0.59482747, 0.00650082, 0.59656335, 0.62658423,\n",
       "        0.52066023, 0.2872158 , 0.2088701 , 0.06732068, 0.69724557,\n",
       "        0.19560231, 0.11436741, 0.86569011, 0.94972365, 0.78641728,\n",
       "        0.32315777, 0.44602207, 0.10462607, 0.82668068, 0.32019176,\n",
       "        0.82226791, 0.18496694, 0.84332012, 0.81587768, 0.99335146,\n",
       "        0.29653501, 0.3186559 , 0.62688622],\n",
       "       [0.96882327, 0.42255526, 0.47240306, 0.78296073, 0.48876987,\n",
       "        0.4539781 , 0.39951903, 0.23782251, 0.46525991, 0.47515418,\n",
       "        0.34519528, 0.03562605, 0.38344316, 0.32761471, 0.73655533,\n",
       "        0.90467202, 0.53912625, 0.89451114, 0.78928836, 0.78569885,\n",
       "        0.24071049, 0.28249989, 0.8093728 , 0.9120698 , 0.86997567,\n",
       "        0.90751464, 0.71394338, 0.08695264],\n",
       "       [0.51779499, 0.64514283, 0.46973192, 0.14635687, 0.63467974,\n",
       "        0.37483438, 0.81784094, 0.57169141, 0.68876792, 0.17648791,\n",
       "        0.01430623, 0.91039937, 0.11800907, 0.26588981, 0.5784621 ,\n",
       "        0.16205478, 0.23449939, 0.67605351, 0.80120224, 0.38875573,\n",
       "        0.60554955, 0.165382  , 0.82075359, 0.89329694, 0.820748  ,\n",
       "        0.07783051, 0.90665387, 0.26335769],\n",
       "       [0.94422992, 0.23175729, 0.36413197, 0.63784662, 0.96937478,\n",
       "        0.29077933, 0.21605847, 0.47634745, 0.55837592, 0.03100286,\n",
       "        0.19008089, 0.67383779, 0.77877509, 0.33756413, 0.56450038,\n",
       "        0.92724502, 0.97969237, 0.38458858, 0.68565121, 0.4302439 ,\n",
       "        0.87705556, 0.39895499, 0.78822104, 0.42826774, 0.02436495,\n",
       "        0.14730281, 0.79804286, 0.4290152 ],\n",
       "       [0.85552228, 0.97118319, 0.57016947, 0.4978295 , 0.43139884,\n",
       "        0.9209926 , 0.0479293 , 0.4644188 , 0.60665895, 0.75525343,\n",
       "        0.46934807, 0.34618238, 0.29448025, 0.90623764, 0.37400444,\n",
       "        0.29411109, 0.54863001, 0.05761433, 0.65635912, 0.5107564 ,\n",
       "        0.5705325 , 0.32056234, 0.08634639, 0.2624297 , 0.46370574,\n",
       "        0.46717168, 0.98927509, 0.92458663],\n",
       "       [0.5647908 , 0.9364857 , 0.12661486, 0.23416072, 0.62761128,\n",
       "        0.17036295, 0.62985859, 0.77928591, 0.64088748, 0.38724776,\n",
       "        0.8803955 , 0.04559259, 0.74186482, 0.31545229, 0.77468703,\n",
       "        0.0086869 , 0.38211112, 0.70504898, 0.85374977, 0.13631676,\n",
       "        0.02112736, 0.59097773, 0.88785187, 0.21134569, 0.56346453,\n",
       "        0.19206458, 0.65720809, 0.3459831 ],\n",
       "       [0.45427903, 0.17119628, 0.26273827, 0.27822939, 0.08692497,\n",
       "        0.86254998, 0.98235264, 0.3794836 , 0.49942429, 0.04395572,\n",
       "        0.18658004, 0.983602  , 0.84339002, 0.79614982, 0.28553367,\n",
       "        0.6983021 , 0.93240754, 0.55306295, 0.96499411, 0.11816029,\n",
       "        0.05438414, 0.93319532, 0.19681371, 0.56871416, 0.26688243,\n",
       "        0.82768373, 0.57105666, 0.58934953],\n",
       "       [0.27544089, 0.64030909, 0.82335815, 0.7568042 , 0.45784329,\n",
       "        0.38830913, 0.69156953, 0.74874748, 0.33405482, 0.78375441,\n",
       "        0.28671868, 0.09247479, 0.51143979, 0.51615835, 0.91189215,\n",
       "        0.5919457 , 0.91049432, 0.65568556, 0.49461802, 0.40093102,\n",
       "        0.6127876 , 0.67581826, 0.1919035 , 0.13822378, 0.31655172,\n",
       "        0.5375732 , 0.68990481, 0.2432159 ],\n",
       "       [0.72458152, 0.01556045, 0.88448272, 0.0415451 , 0.68079057,\n",
       "        0.27222612, 0.31053817, 0.63788211, 0.43086695, 0.86293218,\n",
       "        0.71232212, 0.72215215, 0.69350719, 0.99049017, 0.64428634,\n",
       "        0.35562854, 0.30763178, 0.74942433, 0.53651553, 0.45801257,\n",
       "        0.11736019, 0.68750618, 0.68857023, 0.2858637 , 0.14854549,\n",
       "        0.38411905, 0.35615701, 0.39317973],\n",
       "       [0.36135383, 0.69976381, 0.87623223, 0.53155184, 0.0451593 ,\n",
       "        0.59156238, 0.24251363, 0.59408334, 0.03248232, 0.42676361,\n",
       "        0.59659254, 0.76379664, 0.85416365, 0.9974107 , 0.40968607,\n",
       "        0.28792859, 0.02347859, 0.59800896, 0.7184491 , 0.94994382,\n",
       "        0.26308299, 0.28111768, 0.4063866 , 0.63376974, 0.33179039,\n",
       "        0.60639058, 0.44223968, 0.06004695],\n",
       "       [0.96818025, 0.68548591, 0.83490661, 0.71474222, 0.0357733 ,\n",
       "        0.39391066, 0.42986289, 0.30766999, 0.42318479, 0.58020886,\n",
       "        0.12756895, 0.44100912, 0.87493038, 0.05774057, 0.5120793 ,\n",
       "        0.64495729, 0.56332244, 0.63865364, 0.03167099, 0.80498103,\n",
       "        0.46296972, 0.17963259, 0.64458987, 0.0438956 , 0.70415518,\n",
       "        0.58433695, 0.77324384, 0.70372387]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 3]\n",
      "[0 2 3 0 0]\n",
      "[[0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 2 0 0 0 0]\n",
      " [0 0 0 3 4 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# numpy padding\n",
    "# 1次元配列\n",
    "a = [2, 3]\n",
    "b = np.pad(a, [1,0], \"constant\")\n",
    "print(b) # 先頭に1個、末尾に0個 0パディング\n",
    "c = np.pad(a, [1,2], \"constant\")\n",
    "print(c) # 先頭に1個、末尾に2個 0パディング\n",
    "\n",
    "# 2次元配列\n",
    "d = [[1,2], [3,4]]\n",
    "e = np.pad(d, [(1,2),(3,4)], \"constant\")\n",
    "print(e) # 行の先頭に1行、行末に2行、左の列に3行、右の列に4行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.zeros((2, 3, 3, 3, 2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# im2col(image to column)\n",
    "\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    # input_data : (dataNum, ch, height, width)\n",
    "    # pad : パディング\n",
    "    \n",
    "    # return col: 2次元配列\n",
    "    \n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    \n",
    "    \n",
    "                             # 個数方向, ch方向, 行方向、 列方向\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad,pad), (pad,pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "    \n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "    \n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    col :\n",
    "    input_shape : 入力データの形状（例：(10, 1, 28, 28)）\n",
    "    filter_h :\n",
    "    filter_w\n",
    "    stride\n",
    "    pad\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "# im2colを使う\n",
    "x1 = np.random.rand(1,3,7,7) # 1個のデータ\n",
    "fil_h = 5\n",
    "fil_w = 5\n",
    "col1 = im2col(x1, fil_h, fil_w, stride=1, pad=0)\n",
    "print(col1.shape)\n",
    "\n",
    "x2 = np.random.rand(10, 3, 7, 7) # 10個のデータ\n",
    "col2 = im2col(x2, fil_h, fil_w, stride=1, pad=0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolutionレイヤ\n",
    "class Convolution:\n",
    "    # フィルターサイズが重みW\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # backward用\n",
    "        self.x = None\n",
    "        self.col = None\n",
    "        self.col_w = None\n",
    "        \n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # xは４次元データ\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int((H + 2*self.pad - FH)/self.stride + 1)\n",
    "        out_w = int((W + 2*self.pad - FW)/self.stride + 1)\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        # フィルタの個数は入力データN個(バッチサイズ)でFN個のみ\n",
    "        col_W = self.W.reshape(FN, -1).T # フィルターの展開 # N個のそれぞれの1つ目のフィルタによるブロックは順番に行に挿入される\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        \n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2) # (N, FN, out_h, out_w)\n",
    "        \n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        # dout(N, FN, out_h, out_w) -> (N, out_h, out_w, FN)\n",
    "        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN) # (N, FN, out_h, out_w)を(#(N*分割ブロック数), FN)に変換\n",
    "        \n",
    "        self.db = np.sum(dout, axis=0) # バッチ内の個数単位で合計\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "        \n",
    "        dcol = np.dot(dout, self.col_W.T) # 2次元\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad) # 4次元\n",
    "        \n",
    "        # Convolution内部では4次元->2次元->2次元\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Poolingレイヤ\n",
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # backward用\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int((H - self.pool_h)/self.stride + 1)\n",
    "        out_w = int((W - self.pool_w)/self.stride + 1)\n",
    "        \n",
    "        # 展開(1)\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w) # 1チャネルの1フィルタの値が列をなす行を作る\n",
    "        \n",
    "        # 最大値(2)\n",
    "        out = np.max(col, axis=1)\n",
    "        \n",
    "        # backward用\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        self.arg_max = arg_max\n",
    "        \n",
    "        # 整形(3)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        self.x = x\n",
    "        \n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1) # (N, C, out_h, out_w) -> (N, out_h, out_w, C)\n",
    "\n",
    "        pool_size = self.pool_h*self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size)) # 1列の個数は1フィルタブロック*N\n",
    "\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,))\n",
    "\n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 4, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randn(2, 3, 4, 2)\n",
    "print(x.shape)\n",
    "x.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-2.45271193, -0.97364744],\n",
       "         [ 0.54538116, -1.50692132],\n",
       "         [-0.33446808,  0.07275991],\n",
       "         [-0.14843256, -1.12671648]],\n",
       "\n",
       "        [[ 0.43428211, -0.1616315 ],\n",
       "         [ 1.55441513,  1.03600255],\n",
       "         [-0.84353015, -0.63776027],\n",
       "         [ 1.26770957, -0.85110847]],\n",
       "\n",
       "        [[ 0.64433424, -0.30993471],\n",
       "         [-0.11469489,  0.94370336],\n",
       "         [ 0.75526105, -1.37770427],\n",
       "         [ 0.43196037, -0.09814835]]],\n",
       "\n",
       "\n",
       "       [[[ 0.99672789, -0.24995163],\n",
       "         [-0.14984452, -0.07201555],\n",
       "         [-0.19426536, -0.5466863 ],\n",
       "         [-1.06022015, -0.33362985]],\n",
       "\n",
       "        [[ 0.56152968,  0.87063086],\n",
       "         [-2.18383873,  1.58048642],\n",
       "         [ 1.07670277, -0.60168269],\n",
       "         [ 0.5722494 , -1.46550435]],\n",
       "\n",
       "        [[ 0.51728742,  1.02556607],\n",
       "         [-0.61469622, -1.03680352],\n",
       "         [-1.39902381,  0.00622228],\n",
       "         [-0.34442495,  1.61176446]]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.45271193, -0.97364744,  0.54538116, -1.50692132, -0.33446808,\n",
       "        0.07275991, -0.14843256, -1.12671648,  0.43428211, -0.1616315 ,\n",
       "        1.55441513,  1.03600255, -0.84353015, -0.63776027,  1.26770957,\n",
       "       -0.85110847,  0.64433424, -0.30993471, -0.11469489,  0.94370336,\n",
       "        0.75526105, -1.37770427,  0.43196037, -0.09814835,  0.99672789,\n",
       "       -0.24995163, -0.14984452, -0.07201555, -0.19426536, -0.5466863 ,\n",
       "       -1.06022015, -0.33362985,  0.56152968,  0.87063086, -2.18383873,\n",
       "        1.58048642,  1.07670277, -0.60168269,  0.5722494 , -1.46550435,\n",
       "        0.51728742,  1.02556607, -0.61469622, -1.03680352, -1.39902381,\n",
       "        0.00622228, -0.34442495,  1.61176446])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def affine(x, w, b):\n",
    "    return np.dot(x, w) + b\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "        # テンソル対応\n",
    "        self.original_x_shape = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #self.x = x\n",
    "        \n",
    "        # テンソル対応\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1) # 4次元を2次元に変換\n",
    "        self.x = x\n",
    "        \n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW= np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        # 入力データの形状に戻す（テンソル対応）\n",
    "        dx = dx.reshape(*self.original_x_shape)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.max(0, x)\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, z):\n",
    "        self.mask = (z <= 0)\n",
    "        out = z.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # Reluレイヤのforward入力とbackward出力は同じ次元\n",
    "        dout[self.mask] = 0\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T\n",
    "    \n",
    "    x = x - np.max(x) # オーバーフロー対策\n",
    "    y = np.exp(x) / np.sum(np.exp(x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    \"\"\"\n",
    "    # 交差エントロピーの入力(softmaxの出力)は1xN\n",
    "    if y.ndim == 1: # 1サンプルのみのとき\n",
    "        z = z.reshape(1, z.size) # (*,) -> (1,*)に変更\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    #print(\"y.shape\", y.shape)\n",
    "    #print(\"t.shape\", t.shape)\n",
    "    #print(\"y.size\", y.size)\n",
    "    #print(\"t.size\", t.size)\n",
    "    # テストデータがラベリングのとき\n",
    "    if y.size != t.size:\n",
    "        t_one_hot = np.zeros_like(y, dtype=np.int) # one_hot表現の雛形\n",
    "        for index in range(batch_size):\n",
    "            #print('index', index)\n",
    "            #print('t[index]', t[index])\n",
    "            t_one_hot[index, int(t[index])] = 1\n",
    "        t = t_one_hot # 変換\n",
    "    \n",
    "    # tをone-hot表現に変換してから交差エントロピーを計算する\n",
    "    return -np.sum(t * np.log(y)) / batch_size \n",
    "    \"\"\"\n",
    "    # 以下、サンプル通り\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    #print('t:', t)        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # Softmaxの出力\n",
    "        self.t = None\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.x = x\n",
    "        self.y = softmax(x)\n",
    "        \n",
    "        #print('self.y', self.y.shape)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        \n",
    "        # テストデータがone-hot-vector表現の場合\n",
    "        if self.t.size == self.y.size:\n",
    "            dx = (self.y - self.t) / batch_size # サンプル1個あたりの誤差勾配\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "            \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNNの実装\n",
    "# [Convolution - ReLU - Pooling - Affine - ReLU - Affine - Softmax]\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "class SimpleConvNet:\n",
    "    # input_dim=(チャネル、画像の高さ, 画像の幅)\n",
    "    def __init__(self, \n",
    "                 input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100,\n",
    "                 output_size=10,\n",
    "                 weight_init_std=0.01):\n",
    "        \n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        \n",
    "        input_size = input_dim[1] # Height and Width of img\n",
    "        conv_output_size = (input_size + 2*filter_pad - filter_size)/filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "        \n",
    "        # 重みパラメータの初期化を行うパート\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size) # 入力数, 出力数\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        \n",
    "        # レイヤの作成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], \n",
    "                                           self.params['b1'], \n",
    "                                           conv_param['stride'], \n",
    "                                           conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    \n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "    \n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（数値微分）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "            \n",
    "        return grads\n",
    "    \n",
    "    \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "            \n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Momentum\n",
    "# v <- αV - η*dL/dW\n",
    "# W <- W + v\n",
    "# vは速度, -η*dL/dWが加速度\n",
    "# 勾配がきつい所ほど、更新の値が大きい\n",
    "# α 減速項\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AdaGrad\n",
    "# learning rate decay 徐々に学習係数を小さくする\n",
    "# 学習係数を全体で一括して下げるのではなく、一つ一つのパラメータに対して個々で減衰させる\n",
    "# h <- dL/dW ⦿ dL/dW ※⦿は要素毎の掛け算 つまりhは勾配の２乗和を加算したもの\n",
    "# W <- W - (η/√h) * dL/dW\n",
    "# hは学習が進むに連れて、大きくなるが逓減する\n",
    "# 1/√hはh=0で無限大、hが増加するに連れて小さくなる\n",
    "\n",
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adam ≒　Momentum + AdaGrad\n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.h = 1e-7 # ゼロ割防止の為の微小値\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "\n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "\n",
    "        for key, val in params.items():\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + self.h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RMSprop:\n",
    "    def __init__(self, lr=0.01, decay_rate=0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        self.e = 1e-7 # ゼロ割を防ぐための微小値\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.h[key] *= self.decay_rate\n",
    "            self.h[key] += (1.0 - self.decay_rate) * grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + self.e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Nesterov:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.v[key] *= self.v[key]\n",
    "            self.v[key] -= self.lr * grads[key]\n",
    "            params[key] += self.momentum * self.momentum * self.v[key]\n",
    "            params[key] -= (1.0 + self.momentum) * self.lr * self.v[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "from mnist import load_mnist\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減 \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"ニューラルネットの訓練を行うクラス\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=20, mini_batch_size=100,\n",
    "                 optimizer='SGD', optimizer_param={'lr':0.01}, \n",
    "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
    "        self.network = network\n",
    "        self.verbose = verbose\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = mini_batch_size\n",
    "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
    "\n",
    "        # optimzer\n",
    "        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n",
    "                                'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam}\n",
    "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
    "        \n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
    "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "        \n",
    "        grads = self.network.gradient(x_batch, t_batch)\n",
    "        self.optimizer.update(self.network.params, grads)\n",
    "        \n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "        if self.verbose: print(\"train loss:\" + str(loss))\n",
    "        \n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "            \n",
    "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
    "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
    "            if not self.evaluate_sample_num_per_epoch is None:\n",
    "                t = self.evaluate_sample_num_per_epoch\n",
    "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
    "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
    "                \n",
    "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
    "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "\n",
    "            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "\n",
    "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2987308863816347\n",
      "=== epoch:1, train acc:0.271, test acc:0.217 ===\n",
      "train loss:2.2966087278608907\n",
      "train loss:2.292173087268165\n",
      "train loss:2.288319501856595\n",
      "train loss:2.2797741956333972\n",
      "train loss:2.2646118442708927\n",
      "train loss:2.2490996564925383\n",
      "train loss:2.2265329470595274\n",
      "train loss:2.209158381681098\n",
      "train loss:2.1619132978922067\n",
      "train loss:2.1556923698067187\n",
      "train loss:2.1406208700333096\n",
      "train loss:2.0607414525402747\n",
      "train loss:1.991675404119468\n",
      "train loss:1.9634372279918164\n",
      "train loss:1.8694528787857112\n",
      "train loss:1.814351391251882\n",
      "train loss:1.6727484917487891\n",
      "train loss:1.6016442983716102\n",
      "train loss:1.6472877686388463\n",
      "train loss:1.5556966662394769\n",
      "train loss:1.5152726981640618\n",
      "train loss:1.4833734985124403\n",
      "train loss:1.2076168787001706\n",
      "train loss:1.1885450387800176\n",
      "train loss:1.1045256804736352\n",
      "train loss:1.0962516824850197\n",
      "train loss:1.092655406117911\n",
      "train loss:0.9179880537771623\n",
      "train loss:0.8157958754403026\n",
      "train loss:0.8273218470835663\n",
      "train loss:0.9329024752289024\n",
      "train loss:0.6895026384171358\n",
      "train loss:0.7028929707614852\n",
      "train loss:0.758439335984166\n",
      "train loss:0.6956442008476051\n",
      "train loss:0.7184424956370462\n",
      "train loss:0.6225990886356566\n",
      "train loss:0.693324405886781\n",
      "train loss:0.8083828134396385\n",
      "train loss:0.7191810805695611\n",
      "train loss:0.7517566788102026\n",
      "train loss:0.523628942828611\n",
      "train loss:0.5394982292414915\n",
      "train loss:0.4655487928828532\n",
      "train loss:0.5981266139270981\n",
      "train loss:0.5231707806675895\n",
      "train loss:0.5138110833490034\n",
      "train loss:0.5996544706055734\n",
      "train loss:0.5790144456508525\n",
      "train loss:0.6000873141310941\n",
      "=== epoch:2, train acc:0.815, test acc:0.802 ===\n",
      "train loss:0.6426622655277378\n",
      "train loss:0.542992221639849\n",
      "train loss:0.661330582830448\n",
      "train loss:0.4896414185753061\n",
      "train loss:0.5118058008629627\n",
      "train loss:0.6967483355246581\n",
      "train loss:0.5159547488924692\n",
      "train loss:0.2947605632958306\n",
      "train loss:0.5746513772428955\n",
      "train loss:0.4083619596770957\n",
      "train loss:0.5287251187171894\n",
      "train loss:0.4191500992790148\n",
      "train loss:0.46232586418901944\n",
      "train loss:0.5362155432496047\n",
      "train loss:0.4261362814650023\n",
      "train loss:0.4706096024417462\n",
      "train loss:0.41409898872395273\n",
      "train loss:0.44912204545669177\n",
      "train loss:0.27240812735182174\n",
      "train loss:0.48113230854314487\n",
      "train loss:0.361585293996848\n",
      "train loss:0.3459364874797992\n",
      "train loss:0.4119790097525647\n",
      "train loss:0.4023529956840411\n",
      "train loss:0.3753677331252244\n",
      "train loss:0.25749217624511933\n",
      "train loss:0.33455671126097963\n",
      "train loss:0.48712734618538406\n",
      "train loss:0.37640764149786277\n",
      "train loss:0.3489073818457144\n",
      "train loss:0.2951840409399798\n",
      "train loss:0.3174955581619241\n",
      "train loss:0.393887591192629\n",
      "train loss:0.5089797833687131\n",
      "train loss:0.28676619992287855\n",
      "train loss:0.3322120946781547\n",
      "train loss:0.4159122364856785\n",
      "train loss:0.4232954552025406\n",
      "train loss:0.658120446861963\n",
      "train loss:0.3610593553639655\n",
      "train loss:0.367460433255451\n",
      "train loss:0.23928703323034878\n",
      "train loss:0.48361744219340325\n",
      "train loss:0.24800616988562002\n",
      "train loss:0.48731310620598217\n",
      "train loss:0.42723200480140433\n",
      "train loss:0.4859343662896366\n",
      "train loss:0.39205539240402026\n",
      "train loss:0.364315624310906\n",
      "train loss:0.27513386275616913\n",
      "=== epoch:3, train acc:0.859, test acc:0.861 ===\n",
      "train loss:0.38186490996343403\n",
      "train loss:0.2536165410316469\n",
      "train loss:0.4356448699489925\n",
      "train loss:0.35473510042321243\n",
      "train loss:0.2517643986886518\n",
      "train loss:0.2647079242329313\n",
      "train loss:0.2952303408665522\n",
      "train loss:0.41917416801468105\n",
      "train loss:0.2888134167750192\n",
      "train loss:0.4207069623000271\n",
      "train loss:0.21625963693473174\n",
      "train loss:0.3236193147182524\n",
      "train loss:0.24825016108690764\n",
      "train loss:0.4986233518837336\n",
      "train loss:0.4553240244927327\n",
      "train loss:0.4225744206714173\n",
      "train loss:0.23004795423865165\n",
      "train loss:0.3588436261534353\n",
      "train loss:0.37818447886944495\n",
      "train loss:0.2837845640324214\n",
      "train loss:0.3010976006246295\n",
      "train loss:0.2780943388795147\n",
      "train loss:0.31828445363062674\n",
      "train loss:0.30903528673598557\n",
      "train loss:0.19045890775492802\n",
      "train loss:0.458251184066289\n",
      "train loss:0.32187542488786147\n",
      "train loss:0.3501827744889856\n",
      "train loss:0.1463004430967939\n",
      "train loss:0.17019771168186296\n",
      "train loss:0.3794890916148645\n",
      "train loss:0.31497718082598747\n",
      "train loss:0.3257290369967232\n",
      "train loss:0.29960919184273943\n",
      "train loss:0.19231127602509987\n",
      "train loss:0.20673879551629068\n",
      "train loss:0.2876204158177972\n",
      "train loss:0.31416763528779107\n",
      "train loss:0.2584021598995727\n",
      "train loss:0.22797625194783755\n",
      "train loss:0.30852426730625776\n",
      "train loss:0.24053999634910825\n",
      "train loss:0.35040919731413417\n",
      "train loss:0.23762853629407343\n",
      "train loss:0.20045446055242575\n",
      "train loss:0.28361715210432403\n",
      "train loss:0.2444214571241996\n",
      "train loss:0.3132108556509308\n",
      "train loss:0.12634574800671075\n",
      "train loss:0.2397310537826289\n",
      "=== epoch:4, train acc:0.89, test acc:0.87 ===\n",
      "train loss:0.3213151863812691\n",
      "train loss:0.5611083789096929\n",
      "train loss:0.21044358598535845\n",
      "train loss:0.24421910263645277\n",
      "train loss:0.16809605188377066\n",
      "train loss:0.36885628791483716\n",
      "train loss:0.19855016200893952\n",
      "train loss:0.2984238597043603\n",
      "train loss:0.23082560709044983\n",
      "train loss:0.44336589504322865\n",
      "train loss:0.2773840478012298\n",
      "train loss:0.33091463562320406\n",
      "train loss:0.20681670920582662\n",
      "train loss:0.2439811630285809\n",
      "train loss:0.19882550768369633\n",
      "train loss:0.2209635244241838\n",
      "train loss:0.31970107450145796\n",
      "train loss:0.38379032891899323\n",
      "train loss:0.2624417305267558\n",
      "train loss:0.27745423388581186\n",
      "train loss:0.4027059063449755\n",
      "train loss:0.3559645041469862\n",
      "train loss:0.2619016486468132\n",
      "train loss:0.2882028913458631\n",
      "train loss:0.19495630972574268\n",
      "train loss:0.22522291146547474\n",
      "train loss:0.19701879640402126\n",
      "train loss:0.34920456661581695\n",
      "train loss:0.24238267354811618\n",
      "train loss:0.36179192408246785\n",
      "train loss:0.34589788308832226\n",
      "train loss:0.20010222698820745\n",
      "train loss:0.3091992243664307\n",
      "train loss:0.44300596401588066\n",
      "train loss:0.21675327993724497\n",
      "train loss:0.3544480321468311\n",
      "train loss:0.41206923782043825\n",
      "train loss:0.3058055053707855\n",
      "train loss:0.32448220301170055\n",
      "train loss:0.3243117130161937\n",
      "train loss:0.23488678766005897\n",
      "train loss:0.3506237434560521\n",
      "train loss:0.5795026159574166\n",
      "train loss:0.12568885795062715\n",
      "train loss:0.19269133643401934\n",
      "train loss:0.3136704777026192\n",
      "train loss:0.15583201972719218\n",
      "train loss:0.38196917490473603\n",
      "train loss:0.26434210259263496\n",
      "train loss:0.3047352409971531\n",
      "=== epoch:5, train acc:0.906, test acc:0.891 ===\n",
      "train loss:0.26024322891143337\n",
      "train loss:0.2341667619059836\n",
      "train loss:0.18340461634594843\n",
      "train loss:0.24125659951423695\n",
      "train loss:0.2818962762346778\n",
      "train loss:0.4291634283228211\n",
      "train loss:0.25511271202490104\n",
      "train loss:0.29465303736598114\n",
      "train loss:0.3226628435062399\n",
      "train loss:0.19259473954107376\n",
      "train loss:0.24920245132044305\n",
      "train loss:0.2445854438448392\n",
      "train loss:0.15247900021535415\n",
      "train loss:0.3288528176799538\n",
      "train loss:0.3314676778283778\n",
      "train loss:0.21920606770656875\n",
      "train loss:0.2037019537186854\n",
      "train loss:0.2450463580660956\n",
      "train loss:0.16323854881675134\n",
      "train loss:0.19443197055168074\n",
      "train loss:0.2689567859075186\n",
      "train loss:0.217452548475115\n",
      "train loss:0.23480283824058593\n",
      "train loss:0.26097993301766886\n",
      "train loss:0.1923461054324562\n",
      "train loss:0.13357398063737233\n",
      "train loss:0.2632091331759067\n",
      "train loss:0.2979883746701983\n",
      "train loss:0.3115423910920604\n",
      "train loss:0.3804509089715306\n",
      "train loss:0.22092496822205426\n",
      "train loss:0.3068305807820155\n",
      "train loss:0.262743289152098\n",
      "train loss:0.18823926428065277\n",
      "train loss:0.18284040632139942\n",
      "train loss:0.23309850305822605\n",
      "train loss:0.23975364183537873\n",
      "train loss:0.1240348312669911\n",
      "train loss:0.1875641807590404\n",
      "train loss:0.20000416616744357\n",
      "train loss:0.20536532553048978\n",
      "train loss:0.10485400798312831\n",
      "train loss:0.18992595507509782\n",
      "train loss:0.26988625152421386\n",
      "train loss:0.21310206673368914\n",
      "train loss:0.3247584151831892\n",
      "train loss:0.24393663717768907\n",
      "train loss:0.2865935345488308\n",
      "train loss:0.2550158238691603\n",
      "train loss:0.35483507077092263\n",
      "=== epoch:6, train acc:0.919, test acc:0.899 ===\n",
      "train loss:0.2768569246128946\n",
      "train loss:0.19347205335248416\n",
      "train loss:0.3986829900275254\n",
      "train loss:0.2328221564393192\n",
      "train loss:0.10294537727787426\n",
      "train loss:0.24646035245778872\n",
      "train loss:0.16564382518408574\n",
      "train loss:0.2304372850960289\n",
      "train loss:0.1531594834161151\n",
      "train loss:0.21936684697284556\n",
      "train loss:0.23218992803206928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.2850639614431844\n",
      "train loss:0.19106306766345252\n",
      "train loss:0.2353225017922048\n",
      "train loss:0.16218540060466385\n",
      "train loss:0.29659965787402404\n",
      "train loss:0.2192279472211648\n",
      "train loss:0.21498048977670325\n",
      "train loss:0.2695410465551765\n",
      "train loss:0.26087717847567826\n",
      "train loss:0.2604491184226302\n",
      "train loss:0.26429431220332655\n",
      "train loss:0.34384275735874253\n",
      "train loss:0.29272175355456925\n",
      "train loss:0.24432817014915273\n",
      "train loss:0.16386062034889368\n",
      "train loss:0.14978937775314813\n",
      "train loss:0.2516251081256218\n",
      "train loss:0.3160299156629773\n",
      "train loss:0.2756743508991112\n",
      "train loss:0.19118503320887018\n",
      "train loss:0.21424390302676527\n",
      "train loss:0.13343446595046204\n",
      "train loss:0.2000212066630936\n",
      "train loss:0.1414255076742932\n",
      "train loss:0.22606262076338582\n",
      "train loss:0.16925679384358144\n",
      "train loss:0.17811347696856245\n",
      "train loss:0.1383794271605833\n",
      "train loss:0.1505504588944528\n",
      "train loss:0.29748091908162044\n",
      "train loss:0.1701895761194245\n",
      "train loss:0.12861269011850285\n",
      "train loss:0.10307485318237225\n",
      "train loss:0.11890228687426717\n",
      "train loss:0.21419175002821586\n",
      "train loss:0.25346359016002584\n",
      "train loss:0.27846176013185575\n",
      "train loss:0.15099861077688767\n",
      "train loss:0.11198639589274992\n",
      "=== epoch:7, train acc:0.928, test acc:0.907 ===\n",
      "train loss:0.21578604450470756\n",
      "train loss:0.18290336975452065\n",
      "train loss:0.2008618043893156\n",
      "train loss:0.1838513994316728\n",
      "train loss:0.21039170650048092\n",
      "train loss:0.3091754001074301\n",
      "train loss:0.20682008227981616\n",
      "train loss:0.20274234019658788\n",
      "train loss:0.18106969018770858\n",
      "train loss:0.14017846238646758\n",
      "train loss:0.1539553053351111\n",
      "train loss:0.15991069114551223\n",
      "train loss:0.18061514111240057\n",
      "train loss:0.2880873145110961\n",
      "train loss:0.2423070887997541\n",
      "train loss:0.13827558210019056\n",
      "train loss:0.14199870569050452\n",
      "train loss:0.10655326126249796\n",
      "train loss:0.11096648523695628\n",
      "train loss:0.17524081593409818\n",
      "train loss:0.21070753942923814\n",
      "train loss:0.1139108734923559\n",
      "train loss:0.08478643827665293\n",
      "train loss:0.2715079334779549\n",
      "train loss:0.17159095376951053\n",
      "train loss:0.18844332935663505\n",
      "train loss:0.22921133311548914\n",
      "train loss:0.19614279501576778\n",
      "train loss:0.20412975549507784\n",
      "train loss:0.2091251868893251\n",
      "train loss:0.07359925955860819\n",
      "train loss:0.1116920985931031\n",
      "train loss:0.3637641676387729\n",
      "train loss:0.1168679598496768\n",
      "train loss:0.08173402460640881\n",
      "train loss:0.289587093795383\n",
      "train loss:0.08615501250122003\n",
      "train loss:0.17873987824649754\n",
      "train loss:0.10970113419277094\n",
      "train loss:0.1406493686016796\n",
      "train loss:0.08060907018929686\n",
      "train loss:0.14636851596208528\n",
      "train loss:0.2550946449457809\n",
      "train loss:0.20159633011160533\n",
      "train loss:0.06154717268680308\n",
      "train loss:0.15564839320684448\n",
      "train loss:0.1338301391587372\n",
      "train loss:0.1134155148115048\n",
      "train loss:0.13515641474564766\n",
      "train loss:0.3338835308907355\n",
      "=== epoch:8, train acc:0.951, test acc:0.923 ===\n",
      "train loss:0.07386243714045357\n",
      "train loss:0.17374784193560788\n",
      "train loss:0.1988490353238865\n",
      "train loss:0.2436422412575896\n",
      "train loss:0.18956113323734544\n",
      "train loss:0.12563449543189165\n",
      "train loss:0.1737375103052963\n",
      "train loss:0.09176638207756442\n",
      "train loss:0.17587092133974433\n",
      "train loss:0.21579200813473473\n",
      "train loss:0.2291609008327486\n",
      "train loss:0.14110692882419282\n",
      "train loss:0.13954180403180086\n",
      "train loss:0.19414313799934896\n",
      "train loss:0.14729537275984989\n",
      "train loss:0.06931666844353022\n",
      "train loss:0.2253040730105194\n",
      "train loss:0.16376878803457762\n",
      "train loss:0.16090511782631478\n",
      "train loss:0.08452291176982082\n",
      "train loss:0.0914334752292586\n",
      "train loss:0.09908201234794095\n",
      "train loss:0.15612334710964285\n",
      "train loss:0.14781535362779188\n",
      "train loss:0.12073638936413619\n",
      "train loss:0.12876515611615316\n",
      "train loss:0.24128921236962128\n",
      "train loss:0.18374048170930501\n",
      "train loss:0.1991038354814985\n",
      "train loss:0.18611613333545227\n",
      "train loss:0.18902842576142473\n",
      "train loss:0.10980541939969346\n",
      "train loss:0.1905755095293752\n",
      "train loss:0.21788509213666696\n",
      "train loss:0.10450453893698435\n",
      "train loss:0.22974119289617825\n",
      "train loss:0.22672033691686075\n",
      "train loss:0.17100956480892557\n",
      "train loss:0.06716223362126927\n",
      "train loss:0.10767320072246327\n",
      "train loss:0.11535079409940524\n",
      "train loss:0.15917576033305916\n",
      "train loss:0.22031877085493057\n",
      "train loss:0.10715225203645487\n",
      "train loss:0.16174861538678154\n",
      "train loss:0.12253270639695739\n",
      "train loss:0.12413293735862344\n",
      "train loss:0.19990986695919108\n",
      "train loss:0.13844489928366643\n",
      "train loss:0.13973940602430854\n",
      "=== epoch:9, train acc:0.942, test acc:0.914 ===\n",
      "train loss:0.19381939052483613\n",
      "train loss:0.2963885392330408\n",
      "train loss:0.1720685996275334\n",
      "train loss:0.095494554537909\n",
      "train loss:0.12469627767495181\n",
      "train loss:0.07611141866782152\n",
      "train loss:0.23691253059976997\n",
      "train loss:0.11819456236967095\n",
      "train loss:0.11115300753518544\n",
      "train loss:0.13137908109969249\n",
      "train loss:0.2780328699910465\n",
      "train loss:0.08930062019942835\n",
      "train loss:0.11468885919347327\n",
      "train loss:0.14457322979533527\n",
      "train loss:0.09595869977719454\n",
      "train loss:0.09555302374490292\n",
      "train loss:0.04925208589783806\n",
      "train loss:0.1785695190550139\n",
      "train loss:0.12310284104516356\n",
      "train loss:0.056840343607185505\n",
      "train loss:0.176507069533268\n",
      "train loss:0.07979877409866062\n",
      "train loss:0.13153636809818917\n",
      "train loss:0.057136317234472205\n",
      "train loss:0.1387599605024311\n",
      "train loss:0.22896422001589786\n",
      "train loss:0.12274615390679115\n",
      "train loss:0.10671581517165411\n",
      "train loss:0.17564194011385556\n",
      "train loss:0.14463074412621232\n",
      "train loss:0.07993007852339326\n",
      "train loss:0.1452423479168901\n",
      "train loss:0.06335175880542118\n",
      "train loss:0.11197814655000787\n",
      "train loss:0.1567307295266027\n",
      "train loss:0.1992086967770628\n",
      "train loss:0.1384012115763869\n",
      "train loss:0.1303838624080052\n",
      "train loss:0.12752559449321288\n",
      "train loss:0.1885365486700684\n",
      "train loss:0.15787635806445435\n",
      "train loss:0.09585042979484021\n",
      "train loss:0.13796866384275325\n",
      "train loss:0.13838935799840557\n",
      "train loss:0.12239459400796587\n",
      "train loss:0.10294828746571026\n",
      "train loss:0.2309988244331612\n",
      "train loss:0.10635839343677374\n",
      "train loss:0.18252347946266861\n",
      "train loss:0.17230705915027056\n",
      "=== epoch:10, train acc:0.946, test acc:0.924 ===\n",
      "train loss:0.08589705846948392\n",
      "train loss:0.2420747540152687\n",
      "train loss:0.18971619249283628\n",
      "train loss:0.15733546216682093\n",
      "train loss:0.06618079081079042\n",
      "train loss:0.1701392681703343\n",
      "train loss:0.1335887885859266\n",
      "train loss:0.07507148957311907\n",
      "train loss:0.14498103724697103\n",
      "train loss:0.09988847902325622\n",
      "train loss:0.13726534942952937\n",
      "train loss:0.08682917275113747\n",
      "train loss:0.08299016934792715\n",
      "train loss:0.24174552956365541\n",
      "train loss:0.08472786624443149\n",
      "train loss:0.09632949250006875\n",
      "train loss:0.1344999832523648\n",
      "train loss:0.18503605706646398\n",
      "train loss:0.13067626454962109\n",
      "train loss:0.12017337944251517\n",
      "train loss:0.10028373550545307\n",
      "train loss:0.07662045271613856\n",
      "train loss:0.061087168071619756\n",
      "train loss:0.13676271118147684\n",
      "train loss:0.06063588515705502\n",
      "train loss:0.06252382322729581\n",
      "train loss:0.14971012358203986\n",
      "train loss:0.18397448254909624\n",
      "train loss:0.18500443016081156\n",
      "train loss:0.20240234284480807\n",
      "train loss:0.15119087780712856\n",
      "train loss:0.16191924716907152\n",
      "train loss:0.10370004715946336\n",
      "train loss:0.1289598973080333\n",
      "train loss:0.1918345780439891\n",
      "train loss:0.070776110546747\n",
      "train loss:0.19253102456083201\n",
      "train loss:0.08116063487905581\n",
      "train loss:0.16869317051987923\n",
      "train loss:0.1429773033435473\n",
      "train loss:0.06493432535063913\n",
      "train loss:0.10017503275628487\n",
      "train loss:0.05197757231901892\n",
      "train loss:0.14249068885095245\n",
      "train loss:0.08696370531167222\n",
      "train loss:0.10216891398522586\n",
      "train loss:0.32931069309631084\n",
      "train loss:0.1333132381703662\n",
      "train loss:0.14636256192668395\n",
      "train loss:0.10630147449270837\n",
      "=== epoch:11, train acc:0.949, test acc:0.928 ===\n",
      "train loss:0.15292103663742027\n",
      "train loss:0.06627814076454705\n",
      "train loss:0.23170654714975064\n",
      "train loss:0.11399017284287902\n",
      "train loss:0.16176829041294621\n",
      "train loss:0.16416678905224547\n",
      "train loss:0.08089530047943544\n",
      "train loss:0.1064392622406525\n",
      "train loss:0.10797641776683302\n",
      "train loss:0.14076925840743879\n",
      "train loss:0.07372449202799873\n",
      "train loss:0.11890123945603367\n",
      "train loss:0.08348925126640182\n",
      "train loss:0.11354386287849767\n",
      "train loss:0.1713329436018978\n",
      "train loss:0.0881892021210502\n",
      "train loss:0.07796538682537962\n",
      "train loss:0.14440266617698289\n",
      "train loss:0.13051911932043148\n",
      "train loss:0.09641769227061388\n",
      "train loss:0.14500419896961456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.14126987000135227\n",
      "train loss:0.13512852082615015\n",
      "train loss:0.09150061156427687\n",
      "train loss:0.10321537710397279\n",
      "train loss:0.04848165385438916\n",
      "train loss:0.059895963734582676\n",
      "train loss:0.09453561124216446\n",
      "train loss:0.048425557553635715\n",
      "train loss:0.07124974905834777\n",
      "train loss:0.13420924464472236\n",
      "train loss:0.1458283850276938\n",
      "train loss:0.17353027161479498\n",
      "train loss:0.11686165049011692\n",
      "train loss:0.14633510173673067\n",
      "train loss:0.17652725573879188\n",
      "train loss:0.07441532407679978\n",
      "train loss:0.1450142711710761\n",
      "train loss:0.17799264193295466\n",
      "train loss:0.15816998336895785\n",
      "train loss:0.08717073903343418\n",
      "train loss:0.13979540373918087\n",
      "train loss:0.07784783386572239\n",
      "train loss:0.10302191577173651\n",
      "train loss:0.10164554475099384\n",
      "train loss:0.10032834991048796\n",
      "train loss:0.17736874361605828\n",
      "train loss:0.076828107689313\n",
      "train loss:0.2983186021406514\n",
      "train loss:0.30624632014354997\n",
      "=== epoch:12, train acc:0.953, test acc:0.943 ===\n",
      "train loss:0.09160452671847187\n",
      "train loss:0.09484047943110253\n",
      "train loss:0.15415345449201148\n",
      "train loss:0.058327827748441405\n",
      "train loss:0.07162513909005082\n",
      "train loss:0.049041676238139986\n",
      "train loss:0.09460615236367048\n",
      "train loss:0.17823680983613016\n",
      "train loss:0.09537454893208999\n",
      "train loss:0.07598988961143455\n",
      "train loss:0.10943062343746647\n",
      "train loss:0.05561293807313454\n",
      "train loss:0.0963716431947809\n",
      "train loss:0.04669283171320014\n",
      "train loss:0.048981204763730364\n",
      "train loss:0.06465379691186912\n",
      "train loss:0.1178408165919759\n",
      "train loss:0.09078174691247023\n",
      "train loss:0.05349027685099881\n",
      "train loss:0.08983386776383498\n",
      "train loss:0.07240557641536566\n",
      "train loss:0.029983294169143516\n",
      "train loss:0.09609930123539179\n",
      "train loss:0.1339045904191119\n",
      "train loss:0.07272249962666946\n",
      "train loss:0.06669432852688242\n",
      "train loss:0.04748777237630558\n",
      "train loss:0.17404882624588544\n",
      "train loss:0.09412822753544696\n",
      "train loss:0.0889583089818569\n",
      "train loss:0.10945691825153388\n",
      "train loss:0.06605196200925018\n",
      "train loss:0.07080384416141983\n",
      "train loss:0.14043459288418017\n",
      "train loss:0.0739725659022437\n",
      "train loss:0.06517956024887578\n",
      "train loss:0.11796235605818803\n",
      "train loss:0.11112188666784664\n",
      "train loss:0.07108624755514337\n",
      "train loss:0.1296696989313749\n",
      "train loss:0.08767392891937613\n",
      "train loss:0.0956120315129751\n",
      "train loss:0.09147540153844971\n",
      "train loss:0.1073341628386615\n",
      "train loss:0.0974248887512778\n",
      "train loss:0.0747658947065771\n",
      "train loss:0.17293717044070525\n",
      "train loss:0.06326098096931548\n",
      "train loss:0.056214195400378086\n",
      "train loss:0.13569582660908452\n",
      "=== epoch:13, train acc:0.968, test acc:0.939 ===\n",
      "train loss:0.030129149533457755\n",
      "train loss:0.06766872186155183\n",
      "train loss:0.1172107915274502\n",
      "train loss:0.0867719263989067\n",
      "train loss:0.0796027804485422\n",
      "train loss:0.11167558540616028\n",
      "train loss:0.09949017559641839\n",
      "train loss:0.05659557541725864\n",
      "train loss:0.09461320142275768\n",
      "train loss:0.0422173801350188\n",
      "train loss:0.07360483936281527\n",
      "train loss:0.05911126873643254\n",
      "train loss:0.07004244635654212\n",
      "train loss:0.06419759179132634\n",
      "train loss:0.053685128179255796\n",
      "train loss:0.07837711556778239\n",
      "train loss:0.08966131728167989\n",
      "train loss:0.12553859158366185\n",
      "train loss:0.051917111381121525\n",
      "train loss:0.08725357501144457\n",
      "train loss:0.0681019525134055\n",
      "train loss:0.08615253538724561\n",
      "train loss:0.15003008593827377\n",
      "train loss:0.03925812518681717\n",
      "train loss:0.06676677438405544\n",
      "train loss:0.08117346687327798\n",
      "train loss:0.15328572437839935\n",
      "train loss:0.07861565614844411\n",
      "train loss:0.05848986768294668\n",
      "train loss:0.034685404569858316\n",
      "train loss:0.04524076971201211\n",
      "train loss:0.07182966484185498\n",
      "train loss:0.07481908608754846\n",
      "train loss:0.1589868566445052\n",
      "train loss:0.14120999959628594\n",
      "train loss:0.056578377073552656\n",
      "train loss:0.12901992089874262\n",
      "train loss:0.11585998774764747\n",
      "train loss:0.06175626126097246\n",
      "train loss:0.028441232293634048\n",
      "train loss:0.08202871368206945\n",
      "train loss:0.05787664649107506\n",
      "train loss:0.08055567665369619\n",
      "train loss:0.0811822419400104\n",
      "train loss:0.15502135744569986\n",
      "train loss:0.15201069453778657\n",
      "train loss:0.07720537724384013\n",
      "train loss:0.061298312160551996\n",
      "train loss:0.10089723110402607\n",
      "train loss:0.08954479030293722\n",
      "=== epoch:14, train acc:0.968, test acc:0.947 ===\n",
      "train loss:0.08545711258936611\n",
      "train loss:0.06990232492468311\n",
      "train loss:0.0969390046512891\n",
      "train loss:0.09417060416635885\n",
      "train loss:0.0718805889580153\n",
      "train loss:0.026202885261414642\n",
      "train loss:0.19911771695217376\n",
      "train loss:0.04023072775525391\n",
      "train loss:0.07849507261651928\n",
      "train loss:0.06436669340693649\n",
      "train loss:0.06929092350665114\n",
      "train loss:0.10135815258487416\n",
      "train loss:0.0546461282195879\n",
      "train loss:0.05864786528751363\n",
      "train loss:0.12091767136193177\n",
      "train loss:0.08684196426545787\n",
      "train loss:0.04079827902141216\n",
      "train loss:0.103306386847421\n",
      "train loss:0.051814786430145005\n",
      "train loss:0.1031694645588334\n",
      "train loss:0.13356971965745362\n",
      "train loss:0.09949425876145301\n",
      "train loss:0.08259558840600095\n",
      "train loss:0.1018615666261677\n",
      "train loss:0.10513962965829401\n",
      "train loss:0.02611269457384623\n",
      "train loss:0.0971325164210747\n",
      "train loss:0.0785853210698771\n",
      "train loss:0.05693922906715911\n",
      "train loss:0.03646703164519659\n",
      "train loss:0.05529770848252112\n",
      "train loss:0.045427779607803975\n",
      "train loss:0.04976207749751658\n",
      "train loss:0.054485574905548256\n",
      "train loss:0.03948633431051574\n",
      "train loss:0.06158438855502992\n",
      "train loss:0.02645472543018492\n",
      "train loss:0.06899338586408249\n",
      "train loss:0.06293652081051807\n",
      "train loss:0.026280333468934075\n",
      "train loss:0.0690718136803194\n",
      "train loss:0.06397068180288176\n",
      "train loss:0.058095537417414904\n",
      "train loss:0.06824705098138367\n",
      "train loss:0.02572425655285615\n",
      "train loss:0.02297831028483616\n",
      "train loss:0.07517036286260734\n",
      "train loss:0.06211535672226026\n",
      "train loss:0.046050833860365036\n",
      "train loss:0.06221823940035173\n",
      "=== epoch:15, train acc:0.976, test acc:0.947 ===\n",
      "train loss:0.04902015723096186\n",
      "train loss:0.046069510801846354\n",
      "train loss:0.06890089463616769\n",
      "train loss:0.03006304922437105\n",
      "train loss:0.09908370422477215\n",
      "train loss:0.1580497637893018\n",
      "train loss:0.08941217249530331\n",
      "train loss:0.05982083339935654\n",
      "train loss:0.06259243004575236\n",
      "train loss:0.06607694237389505\n",
      "train loss:0.021482186919962783\n",
      "train loss:0.03459894889575983\n",
      "train loss:0.03457120264312398\n",
      "train loss:0.06514590397963037\n",
      "train loss:0.04536578123089587\n",
      "train loss:0.026272006007520533\n",
      "train loss:0.1127064748604361\n",
      "train loss:0.02878415991379453\n",
      "train loss:0.04781133127098322\n",
      "train loss:0.0575593774511007\n",
      "train loss:0.04017488393998594\n",
      "train loss:0.036584657303219636\n",
      "train loss:0.047445595119107405\n",
      "train loss:0.048763982801301904\n",
      "train loss:0.024850225843653262\n",
      "train loss:0.033680228684231435\n",
      "train loss:0.07364679329945009\n",
      "train loss:0.02258139623559333\n",
      "train loss:0.028369564768652288\n",
      "train loss:0.06990676853674806\n",
      "train loss:0.06364659834440106\n",
      "train loss:0.08736975326754332\n",
      "train loss:0.061940297447491896\n",
      "train loss:0.06412094472263677\n",
      "train loss:0.039322083105352464\n",
      "train loss:0.05670868919179736\n",
      "train loss:0.0395892143724939\n",
      "train loss:0.036921016211543936\n",
      "train loss:0.034826919786340546\n",
      "train loss:0.013032145101718568\n",
      "train loss:0.05766143109298485\n",
      "train loss:0.03616147876912884\n",
      "train loss:0.061816808987319886\n",
      "train loss:0.07831362052120078\n",
      "train loss:0.0959406566235749\n",
      "train loss:0.035570624347784845\n",
      "train loss:0.03277341776139966\n",
      "train loss:0.02140108415298267\n",
      "train loss:0.025693220557523357\n",
      "train loss:0.03388230451524478\n",
      "=== epoch:16, train acc:0.976, test acc:0.946 ===\n",
      "train loss:0.03967585290816193\n",
      "train loss:0.04354800822271441\n",
      "train loss:0.040305972305488345\n",
      "train loss:0.06180205768582895\n",
      "train loss:0.018809238817246415\n",
      "train loss:0.060407358661468694\n",
      "train loss:0.03568215546380011\n",
      "train loss:0.13218976948599023\n",
      "train loss:0.024889175319221194\n",
      "train loss:0.07076659461924194\n",
      "train loss:0.03761715358351203\n",
      "train loss:0.06418990346874659\n",
      "train loss:0.06205329737334685\n",
      "train loss:0.02853056254241221\n",
      "train loss:0.07569276387529036\n",
      "train loss:0.021500256202749396\n",
      "train loss:0.0915695546936529\n",
      "train loss:0.043719120735124635\n",
      "train loss:0.030526870060395516\n",
      "train loss:0.058087459303871845\n",
      "train loss:0.0339543418942541\n",
      "train loss:0.05505416987559226\n",
      "train loss:0.042072629022353196\n",
      "train loss:0.0402665972132061\n",
      "train loss:0.023389195487763928\n",
      "train loss:0.04556527449285115\n",
      "train loss:0.031001313426587143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.1137967444651696\n",
      "train loss:0.017617510749506633\n",
      "train loss:0.03963822803689172\n",
      "train loss:0.04334864551282316\n",
      "train loss:0.04088370106385108\n",
      "train loss:0.046762804741415344\n",
      "train loss:0.07314510096897169\n",
      "train loss:0.07153432690980872\n",
      "train loss:0.031055264279654046\n",
      "train loss:0.046215643300072745\n",
      "train loss:0.04043058897265928\n",
      "train loss:0.0461516453807534\n",
      "train loss:0.01937763344060239\n",
      "train loss:0.04010768704127577\n",
      "train loss:0.05612311024920512\n",
      "train loss:0.030031695664608428\n",
      "train loss:0.09147004083753815\n",
      "train loss:0.08291583942090103\n",
      "train loss:0.027740717386860406\n",
      "train loss:0.01671395386088276\n",
      "train loss:0.029951995154909757\n",
      "train loss:0.028659739027530426\n",
      "train loss:0.043586186662504\n",
      "=== epoch:17, train acc:0.986, test acc:0.951 ===\n",
      "train loss:0.022377125504381624\n",
      "train loss:0.047809029432742525\n",
      "train loss:0.04326756007765105\n",
      "train loss:0.028049798588236734\n",
      "train loss:0.03917882542000939\n",
      "train loss:0.059154610113620484\n",
      "train loss:0.043883736624429986\n",
      "train loss:0.06508634524038995\n",
      "train loss:0.12785400069653818\n",
      "train loss:0.07720080650704357\n",
      "train loss:0.05902235653919157\n",
      "train loss:0.04572641686219593\n",
      "train loss:0.030775051450295604\n",
      "train loss:0.033868711278233626\n",
      "train loss:0.14068708480105227\n",
      "train loss:0.04216767952265197\n",
      "train loss:0.07428300893276835\n",
      "train loss:0.05215910314606664\n",
      "train loss:0.021012231598376688\n",
      "train loss:0.05143638782866604\n",
      "train loss:0.09064114919887366\n",
      "train loss:0.058696313788537784\n",
      "train loss:0.062083224065185176\n",
      "train loss:0.018125519790898194\n",
      "train loss:0.03207595253276316\n",
      "train loss:0.03871125806439117\n",
      "train loss:0.029339566532123308\n",
      "train loss:0.028159602669829704\n",
      "train loss:0.09959986542074546\n",
      "train loss:0.08698963315874904\n",
      "train loss:0.027795900990220505\n",
      "train loss:0.043168654934314266\n",
      "train loss:0.04918259474563091\n",
      "train loss:0.05799905230667106\n",
      "train loss:0.018238181472697547\n",
      "train loss:0.03432920065088375\n",
      "train loss:0.03714597880485344\n",
      "train loss:0.07552170603916722\n",
      "train loss:0.06058841829400558\n",
      "train loss:0.035877554614582585\n",
      "train loss:0.052481504635766486\n",
      "train loss:0.019179046118957627\n",
      "train loss:0.033338563006743704\n",
      "train loss:0.058379870036445854\n",
      "train loss:0.03429569857511928\n",
      "train loss:0.01681673753217459\n",
      "train loss:0.06867938152742284\n",
      "train loss:0.02423097522656408\n",
      "train loss:0.07053029256984127\n",
      "train loss:0.023897816170319244\n",
      "=== epoch:18, train acc:0.977, test acc:0.95 ===\n",
      "train loss:0.062356402631669916\n",
      "train loss:0.045268889567919066\n",
      "train loss:0.027827747765520317\n",
      "train loss:0.05899435956287838\n",
      "train loss:0.02390762173329799\n",
      "train loss:0.02884844954359051\n",
      "train loss:0.10619942812015382\n",
      "train loss:0.02480157310163555\n",
      "train loss:0.03418812795585496\n",
      "train loss:0.04770888043542154\n",
      "train loss:0.034849045953367616\n",
      "train loss:0.06163221601211858\n",
      "train loss:0.058328883322168575\n",
      "train loss:0.04060294627805205\n",
      "train loss:0.03239679378294292\n",
      "train loss:0.02345583652011248\n",
      "train loss:0.024117744325319036\n",
      "train loss:0.05094816208068078\n",
      "train loss:0.06349409693357339\n",
      "train loss:0.14179637278605756\n",
      "train loss:0.03078203716633023\n",
      "train loss:0.03546689757214208\n",
      "train loss:0.02125496688646809\n",
      "train loss:0.0384905066509842\n",
      "train loss:0.03956209907849487\n",
      "train loss:0.022655727135166885\n",
      "train loss:0.052884039458301065\n",
      "train loss:0.025371651057192\n",
      "train loss:0.09103201354760111\n",
      "train loss:0.03629177159624292\n",
      "train loss:0.043068938769553346\n",
      "train loss:0.06824918026415022\n",
      "train loss:0.019823244245998656\n",
      "train loss:0.027013035413185824\n",
      "train loss:0.044231956952646125\n",
      "train loss:0.06335394827082665\n",
      "train loss:0.016186030903915582\n",
      "train loss:0.03882655565832085\n",
      "train loss:0.0354479421039679\n",
      "train loss:0.027338236534784382\n",
      "train loss:0.058013202856832514\n",
      "train loss:0.015411097796075288\n",
      "train loss:0.023916585932322466\n",
      "train loss:0.009281025234096015\n",
      "train loss:0.013324525675496192\n",
      "train loss:0.02992950136086507\n",
      "train loss:0.04838047422483566\n",
      "train loss:0.013260658866546635\n",
      "train loss:0.03072957581795766\n",
      "train loss:0.02620946223314629\n",
      "=== epoch:19, train acc:0.991, test acc:0.957 ===\n",
      "train loss:0.02465321711499732\n",
      "train loss:0.023296859979813275\n",
      "train loss:0.02238115577140642\n",
      "train loss:0.0183697223332077\n",
      "train loss:0.02115147890141476\n",
      "train loss:0.01784498151475329\n",
      "train loss:0.03442392023778005\n",
      "train loss:0.02522934200573105\n",
      "train loss:0.01614726151923203\n",
      "train loss:0.07851641996862142\n",
      "train loss:0.04366167396210803\n",
      "train loss:0.030192457399563288\n",
      "train loss:0.020901069841549557\n",
      "train loss:0.0314916665766689\n",
      "train loss:0.008976333659290089\n",
      "train loss:0.08611165007253971\n",
      "train loss:0.07792160281189882\n",
      "train loss:0.042114785852918224\n",
      "train loss:0.0154772892689199\n",
      "train loss:0.1021695422055238\n",
      "train loss:0.023291905022659362\n",
      "train loss:0.04137263878003718\n",
      "train loss:0.0353308927722301\n",
      "train loss:0.030175341680032473\n",
      "train loss:0.06909586015247599\n",
      "train loss:0.030696865709209455\n",
      "train loss:0.022790659324823593\n",
      "train loss:0.047551208038382115\n",
      "train loss:0.02688530465808404\n",
      "train loss:0.014623212500517261\n",
      "train loss:0.034735102771534965\n",
      "train loss:0.03387371459887923\n",
      "train loss:0.01673216831060722\n",
      "train loss:0.028981679429307343\n",
      "train loss:0.1177234625715944\n",
      "train loss:0.08232829856310019\n",
      "train loss:0.03683612852044045\n",
      "train loss:0.0280781214383632\n",
      "train loss:0.009552793553885226\n",
      "train loss:0.02015974435533228\n",
      "train loss:0.015383771841599917\n",
      "train loss:0.052468078521673886\n",
      "train loss:0.009996900159337813\n",
      "train loss:0.02837354071548008\n",
      "train loss:0.022380301658624327\n",
      "train loss:0.02570924350952728\n",
      "train loss:0.03073066698154674\n",
      "train loss:0.07629765751185595\n",
      "train loss:0.032518345390759144\n",
      "train loss:0.058965736774588995\n",
      "=== epoch:20, train acc:0.986, test acc:0.956 ===\n",
      "train loss:0.02123334512078804\n",
      "train loss:0.022349320831072376\n",
      "train loss:0.045802270754189675\n",
      "train loss:0.021078632837912\n",
      "train loss:0.06794853932189932\n",
      "train loss:0.03318490765000293\n",
      "train loss:0.023057895209096804\n",
      "train loss:0.05147748673715788\n",
      "train loss:0.034481514314449985\n",
      "train loss:0.0404174637681832\n",
      "train loss:0.02590767153696528\n",
      "train loss:0.044477244018396256\n",
      "train loss:0.02947147493431058\n",
      "train loss:0.01868539448138778\n",
      "train loss:0.02731221672563728\n",
      "train loss:0.01848647329655049\n",
      "train loss:0.020953440923981596\n",
      "train loss:0.018232654978548524\n",
      "train loss:0.025269736992029165\n",
      "train loss:0.01957433338762802\n",
      "train loss:0.008166086994450263\n",
      "train loss:0.030622075640656318\n",
      "train loss:0.011704513444955133\n",
      "train loss:0.0590250297207112\n",
      "train loss:0.018900633677548583\n",
      "train loss:0.022102145603444083\n",
      "train loss:0.01932368430025178\n",
      "train loss:0.03519822435547595\n",
      "train loss:0.009359849785731253\n",
      "train loss:0.1426900729167453\n",
      "train loss:0.01772447653094261\n",
      "train loss:0.016701494110940313\n",
      "train loss:0.019160808117099443\n",
      "train loss:0.015110203962244793\n",
      "train loss:0.017681072287509005\n",
      "train loss:0.03820124927724382\n",
      "train loss:0.020208069626716407\n",
      "train loss:0.02029413536153957\n",
      "train loss:0.020797717281289284\n",
      "train loss:0.05525910673852047\n",
      "train loss:0.014533167811658056\n",
      "train loss:0.014564065919481584\n",
      "train loss:0.01844024196767237\n",
      "train loss:0.01971048091239896\n",
      "train loss:0.01722026198080152\n",
      "train loss:0.00973076120491985\n",
      "train loss:0.03545306695388792\n",
      "train loss:0.021146292352590877\n",
      "train loss:0.033448169297880914\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.955\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110d83c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcUAAAEjCAYAAABD3BobAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noRQBAHAoRQAAHEoRAADnvwDw6e21eIcqUwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110dc69e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcUAAAEjCAYAAABD3BobAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLxIL4PMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11194deb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "network = SimpleConvNet()\n",
    "# ランダム初期化後の重み\n",
    "filter_show(network.params['W1'])\n",
    "\n",
    "# 学習後の重み\n",
    "network.load_params(\"params.pkl\")\n",
    "filter_show(network.params['W1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
